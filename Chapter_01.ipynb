{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYraH8CMnWcx4JfUT7Wsoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gowtham-Pentela/NLP/blob/main/Chapter_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I-WNmXR4_S9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783b2743-818c-4af5-8057-8a767b7b221e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: poetry in /usr/local/lib/python3.12/dist-packages (2.3.2)\n",
            "Requirement already satisfied: build<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.4.0)\n",
            "Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (0.14.4)\n",
            "Requirement already satisfied: cleo<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.1.0)\n",
            "Requirement already satisfied: dulwich<2,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.1.0)\n",
            "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.21.2)\n",
            "Requirement already satisfied: findpython<0.8.0,>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from poetry) (0.7.1)\n",
            "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (0.7.0)\n",
            "Requirement already satisfied: keyring<26.0.0,>=25.1.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (25.7.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from poetry) (26.0)\n",
            "Requirement already satisfied: pbs-installer>=2025.6.10 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]>=2025.6.10->poetry) (2026.2.11)\n",
            "Requirement already satisfied: pkginfo<2.0,>=1.12 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.12.1.2)\n",
            "Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (4.9.2)\n",
            "Requirement already satisfied: poetry-core==2.3.1 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.3.1)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.2.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.26 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.0.0)\n",
            "Requirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.5.4)\n",
            "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /usr/local/lib/python3.12/dist-packages (from poetry) (0.13.3)\n",
            "Requirement already satisfied: trove-classifiers>=2022.5.19 in /usr/local/lib/python3.12/dist-packages (from poetry) (2026.1.14.14)\n",
            "Requirement already satisfied: virtualenv>=20.26.6 in /usr/local/lib/python3.12/dist-packages (from poetry) (20.39.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (1.1.2)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (3.24.2)\n",
            "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from cleo<3.0.0,>=2.1.0->poetry) (0.4.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from cleo<3.0.0,>=2.1.0->poetry) (3.14.3)\n",
            "Requirement already satisfied: urllib3>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from dulwich<2,>=0.25.0->poetry) (2.5.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.5.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (4.4.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (6.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]>=2025.6.10->poetry) (0.28.1)\n",
            "Requirement already satisfied: zstandard>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]>=2025.6.10->poetry) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (2026.1.4)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.26.6->poetry) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (0.16.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (2.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (3.0)\n",
            "fatal: destination path 'Python-Natural-Language-Processing-Cookbook-Second-Edition' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install poetry\n",
        "!git clone \"https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition.git\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/\"\n"
      ],
      "metadata": {
        "id": "4uKuHGL4Ex7B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run -i '/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/util/file_utils.ipynb'\n",
        "\n"
      ],
      "metadata": {
        "id": "9t-d9NWJbMDq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBIvJcMdcQ3l",
        "outputId": "f6e7f3df-41e8-47d5-b4a4-3017691e6450"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring appnope: markers 'python_version >= \"3.9\" and python_version < \"4.0\" and platform_system == \"Darwin\" or python_version >= \"3.9\" and python_version < \"4.0\" and sys_platform == \"darwin\"' don't match your environment\n",
            "Ignoring exceptiongroup: markers 'python_version >= \"3.9\" and python_version < \"3.11\"' don't match your environment\n",
            "Ignoring importlib-metadata: markers 'python_version >= \"3.9\" and python_version < \"3.10\"' don't match your environment\n",
            "Ignoring importlib-resources: markers 'python_version >= \"3.9\" and python_version < \"3.10\"' don't match your environment\n",
            "Ignoring pywin32: markers 'sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" and python_version >= \"3.9\" and python_version < \"4.0\"' don't match your environment\n",
            "Ignoring pywinpty: markers 'python_version >= \"3.9\" and python_version < \"4.0\" and os_name == \"nt\"' don't match your environment\n",
            "Ignoring tomli: markers 'python_version >= \"3.9\" and python_full_version <= \"3.11.0a6\"' don't match your environment\n",
            "Ignoring zipp: markers 'python_version >= \"3.9\" and python_version < \"3.10\"' don't match your environment\n",
            "Collecting accelerate==0.24.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 1))\n",
            "  Using cached accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting aiohttp==3.8.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 2))\n",
            "  Using cached aiohttp-3.8.5.tar.gz (7.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiosignal==1.3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 3))\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting alabaster==0.7.13 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 4))\n",
            "  Using cached alabaster-0.7.13-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting annotated-types==0.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 5))\n",
            "  Using cached annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting anyio==4.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 6))\n",
            "  Using cached anyio-4.0.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting argon2-cffi-bindings==21.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 8))\n",
            "  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting argon2-cffi==23.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 9))\n",
            "  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting arrow==1.2.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 10))\n",
            "  Using cached arrow-1.2.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting astatine==0.3.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 11))\n",
            "  Using cached astatine-0.3.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting astor==0.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 12))\n",
            "  Using cached astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting astpretty==3.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 13))\n",
            "  Using cached astpretty-3.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting astroid==3.0.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 14))\n",
            "  Using cached astroid-3.0.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting asttokens==2.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 15))\n",
            "  Using cached asttokens-2.4.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting async-lru==2.0.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 16))\n",
            "  Using cached async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting async-timeout==4.0.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 17))\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting attrs==23.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 18))\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autoflake==1.7.8 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 19))\n",
            "  Using cached autoflake-1.7.8-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting babel==2.12.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 20))\n",
            "  Using cached Babel-2.12.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 21)) (0.2.0)\n",
            "Collecting bandit==1.7.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 22))\n",
            "  Using cached bandit-1.7.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting beautifulsoup4==4.12.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 23))\n",
            "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting bertopic==0.16.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 24))\n",
            "  Using cached bertopic-0.16.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting black==23.11.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 25))\n",
            "  Using cached black-23.11.0-py3-none-any.whl.metadata (66 kB)\n",
            "Collecting bleach==6.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 26))\n",
            "  Using cached bleach-6.0.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting blis==0.7.10 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 27))\n",
            "  Using cached blis-0.7.10.tar.gz (2.9 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cachetools==5.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 28))\n",
            "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting catalogue==2.0.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 29))\n",
            "  Using cached catalogue-2.0.9-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting certifi==2023.7.22 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 30))\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cffi==1.15.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 31))\n",
            "  Using cached cffi-1.15.1.tar.gz (508 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet==5.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 32)) (5.2.0)\n",
            "Collecting charset-normalizer==3.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 33))\n",
            "  Using cached charset_normalizer-3.2.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting click==8.1.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 34))\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cmake==3.27.4.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 35))\n",
            "  Using cached cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting cognitive-complexity==1.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 36))\n",
            "  Using cached cognitive_complexity-1.3.0.tar.gz (5.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama==0.4.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 37))\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting comm==0.1.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 38))\n",
            "  Using cached comm-0.1.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting confection==0.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 39))\n",
            "  Using cached confection-0.1.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting contextualized-topic-models==2.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 40))\n",
            "  Using cached contextualized_topic_models-2.5.0-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Collecting contourpy==1.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 41))\n",
            "  Using cached contourpy-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting coverage==7.3.2 (from coverage[toml]==7.3.2->-r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 42))\n",
            "  Using cached coverage-7.3.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 43)) (0.12.1)\n",
            "Collecting cymem==2.0.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 44))\n",
            "  Using cached cymem-2.0.7.tar.gz (9.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cython==0.29.36 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 45))\n",
            "  Using cached Cython-0.29.36-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting darglint==1.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 46))\n",
            "  Using cached darglint-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dataclasses-json==0.5.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 47))\n",
            "  Using cached dataclasses_json-0.5.9-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting datasets==2.14.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 48))\n",
            "  Using cached datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting debugpy==1.6.7.post1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 49))\n",
            "  Using cached debugpy-1.6.7.post1-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting decorator==5.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 50))\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 51)) (0.7.1)\n",
            "Collecting dill==0.3.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 52))\n",
            "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting distlib==0.3.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 53))\n",
            "  Using cached distlib-0.3.7-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 54)) (1.9.0)\n",
            "Collecting dlint==0.14.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 55))\n",
            "  Using cached dlint-0.14.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting doc8==1.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 56))\n",
            "  Using cached doc8-1.1.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting docformatter==1.7.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 57))\n",
            "  Using cached docformatter-1.7.5-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting docutils==0.19 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 58))\n",
            "  Using cached docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting domdf-python-tools==3.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 59))\n",
            "  Using cached domdf_python_tools-3.7.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting eradicate==2.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 60))\n",
            "  Using cached eradicate-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting evaluate==0.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 61))\n",
            "  Using cached evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting executing==1.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 63))\n",
            "  Using cached executing-1.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting fastapi==0.103.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 64))\n",
            "  Using cached fastapi-0.103.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting fastjsonschema==2.18.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 65))\n",
            "  Using cached fastjsonschema-2.18.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting filelock==3.12.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 66))\n",
            "  Using cached filelock-3.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting flake8-2020==1.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 67))\n",
            "  Using cached flake8_2020-1.8.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting flake8-aaa==0.17.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 68))\n",
            "  Using cached flake8_aaa-0.17.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting flake8-annotations-complexity==0.0.8 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 69))\n",
            "  Using cached flake8_annotations_complexity-0.0.8-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-annotations-coverage==0.0.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 70))\n",
            "  Using cached flake8_annotations_coverage-0.0.6-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting flake8-annotations==3.0.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 71))\n",
            "  Using cached flake8_annotations-3.0.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-bandit==4.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 72))\n",
            "  Using cached flake8_bandit-4.1.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-black==0.3.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 73))\n",
            "  Using cached flake8_black-0.3.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-blind-except==0.2.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 74))\n",
            "  Using cached flake8-blind-except-0.2.1.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-breakpoint==1.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 75))\n",
            "  Using cached flake8_breakpoint-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-broken-line==0.6.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 76))\n",
            "  Using cached flake8_broken_line-0.6.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting flake8-bugbear==23.3.12 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 77))\n",
            "  Using cached flake8_bugbear-23.3.12-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting flake8-builtins==1.5.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 78))\n",
            "  Using cached flake8_builtins-1.5.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting flake8-class-attributes-order==0.1.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 79))\n",
            "  Using cached flake8_class_attributes_order-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting flake8-coding==1.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 80))\n",
            "  Using cached flake8_coding-1.3.2-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-cognitive-complexity==0.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 81))\n",
            "  Using cached flake8_cognitive_complexity-0.1.0.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-comments==0.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 82))\n",
            "  Using cached flake8_comments-0.1.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting flake8-comprehensions==3.14.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 83))\n",
            "  Using cached flake8_comprehensions-3.14.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-debugger==4.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 84))\n",
            "  Using cached flake8_debugger-4.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting flake8-django==1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 85))\n",
            "  Using cached flake8_django-1.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-docstrings==1.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 86))\n",
            "  Using cached flake8_docstrings-1.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-encodings==0.5.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 87))\n",
            "  Using cached flake8_encodings-0.5.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting flake8-eradicate==1.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 88))\n",
            "  Using cached flake8_eradicate-1.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flake8-executable==2.1.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 89))\n",
            "  Using cached flake8_executable-2.1.3-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting flake8-expression-complexity==0.0.11 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 90))\n",
            "  Using cached flake8_expression_complexity-0.0.11-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting flake8-fastapi==0.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 91))\n",
            "  Using cached flake8_fastapi-0.7.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting flake8-fixme==1.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 92))\n",
            "  Using cached flake8_fixme-1.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting flake8-functions-names==0.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 93))\n",
            "  Using cached flake8_functions_names-0.4.0.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-functions==0.0.8 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 94))\n",
            "  Using cached flake8_functions-0.0.8-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-future-annotations==0.0.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 95))\n",
            "  Using cached flake8_future_annotations-0.0.5-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-helper==0.2.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 96))\n",
            "  Using cached flake8_helper-0.2.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting flake8-isort==6.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 97))\n",
            "  Using cached flake8_isort-6.1.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting flake8-literal==1.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 98))\n",
            "  Using cached flake8_literal-1.3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting flake8-logging-format==0.9.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 99))\n",
            "  Using cached flake8-logging-format-0.9.0.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-markdown==0.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 100))\n",
            "  Using cached flake8_markdown-0.5.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting flake8-mutable==1.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 101))\n",
            "  Using cached flake8-mutable-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-no-pep420==2.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 102))\n",
            "  Using cached flake8_no_pep420-2.7.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting flake8-noqa==1.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 103))\n",
            "  Using cached flake8_noqa-1.3.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting flake8-pie==0.16.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 104))\n",
            "  Using cached flake8_pie-0.16.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-plugin-utils==1.3.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 105))\n",
            "  Using cached flake8_plugin_utils-1.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting flake8-pyi==22.11.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 106))\n",
            "  Using cached flake8_pyi-22.11.0-py37-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-pylint==0.2.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 107))\n",
            "  Using cached flake8_pylint-0.2.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting flake8-pytest-style==1.7.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 108))\n",
            "  Using cached flake8_pytest_style-1.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-quotes==3.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 109))\n",
            "  Using cached flake8-quotes-3.3.2.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-rst-docstrings==0.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 110))\n",
            "  Using cached flake8_rst_docstrings-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting flake8-secure-coding-standard==1.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 111))\n",
            "  Using cached flake8_secure_coding_standard-1.4.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting flake8-simplify==0.21.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 112))\n",
            "  Using cached flake8_simplify-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting flake8-string-format==0.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 113))\n",
            "  Using cached flake8_string_format-0.3.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting flake8-tidy-imports==4.10.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 114))\n",
            "  Using cached flake8_tidy_imports-4.10.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting flake8-typing-imports==1.15.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 115))\n",
            "  Using cached flake8_typing_imports-1.15.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-use-fstring==1.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 116))\n",
            "  Using cached flake8-use-fstring-1.4.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-use-pathlib==0.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 117))\n",
            "  Using cached flake8_use_pathlib-0.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting flake8-useless-assert==0.4.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 118))\n",
            "  Using cached flake8_useless_assert-0.4.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting flake8-variables-names==0.0.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 119))\n",
            "  Using cached flake8_variables_names-0.0.6-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting flake8-warnings==0.4.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 120))\n",
            "  Using cached flake8_warnings-0.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flake8==5.0.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 121))\n",
            "  Using cached flake8-5.0.4-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting fonttools==4.43.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 122))\n",
            "  Using cached fonttools-4.43.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
            "Requirement already satisfied: fqdn==1.5.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 123)) (1.5.1)\n",
            "Collecting frozenlist==1.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 124))\n",
            "  Using cached frozenlist-1.4.0.tar.gz (90 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fsspec==2023.6.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 125))\n",
            "  Using cached fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting funcy==2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 127))\n",
            "  Using cached funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting gensim==4.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 128))\n",
            "  Using cached gensim-4.2.0.tar.gz (23.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitdb==4.0.11 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 129))\n",
            "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gitpython==3.1.40 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 130))\n",
            "  Using cached GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting greenlet==2.0.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 131))\n",
            "  Using cached greenlet-2.0.2.tar.gz (164 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h11==0.14.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 132))\n",
            "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting hdbscan==0.8.33 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 133))\n",
            "  Using cached hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpcore==1.0.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 134))\n",
            "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httpx==0.27.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 135))\n",
            "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting huggingface-hub==0.16.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 136))\n",
            "  Using cached huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hypothesis==6.91.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 137))\n",
            "  Using cached hypothesis-6.91.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting hypothesmith==0.1.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 138))\n",
            "  Using cached hypothesmith-0.1.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting idna==3.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 139))\n",
            "  Using cached idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: imagesize==1.4.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 140)) (1.4.1)\n",
            "Collecting iniconfig==2.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 143))\n",
            "  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ipykernel==6.25.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 144))\n",
            "  Using cached ipykernel-6.25.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 145)) (0.2.0)\n",
            "Collecting ipython==8.10.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 146))\n",
            "  Using cached ipython-8.10.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ipywidgets==7.5.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 147))\n",
            "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: isoduration==20.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 148)) (20.11.0)\n",
            "Collecting isort==5.12.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 149))\n",
            "  Using cached isort-5.12.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting jedi==0.19.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 150))\n",
            "  Using cached jedi-0.19.0-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jinja2==3.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 151))\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting joblib==1.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 152))\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting json5==0.9.14 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 153))\n",
            "  Using cached json5-0.9.14-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonpointer==2.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 154))\n",
            "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting jsonschema-specifications==2023.7.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 155))\n",
            "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting jsonschema==4.19.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 156))\n",
            "  Using cached jsonschema-4.19.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jupyter-client==8.3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 158))\n",
            "  Using cached jupyter_client-8.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: jupyter-console==6.6.3 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 159)) (6.6.3)\n",
            "Collecting jupyter-core==5.3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 160))\n",
            "  Using cached jupyter_core-5.3.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting jupyter-events==0.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 161))\n",
            "  Using cached jupyter_events-0.7.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jupyter-lsp==2.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 162))\n",
            "  Using cached jupyter_lsp-2.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server-terminals==0.4.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 163))\n",
            "  Using cached jupyter_server_terminals-0.4.4-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jupyter-server==2.7.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 164))\n",
            "  Using cached jupyter_server-2.7.3-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting jupyter==1.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 165))\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting jupyterlab-flake8==0.7.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 166))\n",
            "  Using cached jupyterlab_flake8-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyterlab-pygments==0.2.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 167))\n",
            "  Using cached jupyterlab_pygments-0.2.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting jupyterlab-server==2.24.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 168))\n",
            "  Using cached jupyterlab_server-2.24.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyterlab==4.0.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 169))\n",
            "  Using cached jupyterlab-4.0.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kiwisolver==1.4.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 170))\n",
            "  Using cached kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting langchain==0.0.284 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 171))\n",
            "  Using cached langchain-0.0.284-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langcodes==3.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 172))\n",
            "  Using cached langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langdetect==1.0.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 173))\n",
            "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langsmith==0.0.33 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 174))\n",
            "  Using cached langsmith-0.0.33-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting lark-parser==0.12.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 175))\n",
            "  Using cached lark_parser-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting levenshtein==0.22.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 176))\n",
            "  Using cached Levenshtein-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting libcst==0.4.10 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 177))\n",
            "  Using cached libcst-0.4.10.tar.gz (752 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lit==16.0.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 178))\n",
            "  Using cached lit-16.0.6.tar.gz (153 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama-index==0.7.24.post1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 179))\n",
            "  Using cached llama_index-0.7.24.post1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting llvmlite==0.41.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 180))\n",
            "  Using cached llvmlite-0.41.1.tar.gz (146 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting markdown-it-py==3.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 181))\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting markupsafe==2.1.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 182))\n",
            "  Using cached MarkupSafe-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting marshmallow-enum==1.5.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 183))\n",
            "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting marshmallow==3.20.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 184))\n",
            "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting matplotlib-inline==0.1.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 185))\n",
            "  Using cached matplotlib_inline-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting matplotlib==3.8.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 186))\n",
            "  Using cached matplotlib-3.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting mccabe==0.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 187))\n",
            "  Using cached mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 188)) (0.1.2)\n",
            "Collecting mistune==3.0.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 189))\n",
            "  Using cached mistune-3.0.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 190)) (1.3.0)\n",
            "Collecting mr-proper==0.0.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 191))\n",
            "  Using cached mr_proper-0.0.7-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting multidict==6.0.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 192))\n",
            "  Using cached multidict-6.0.4.tar.gz (51 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting multiprocess==0.70.15 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 193))\n",
            "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting murmurhash==1.0.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 194))\n",
            "  Using cached murmurhash-1.0.9.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mypy-extensions==1.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 195))\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: natsort==8.4.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 196)) (8.4.0)\n",
            "Collecting nbclient==0.8.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 197))\n",
            "  Using cached nbclient-0.8.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting nbconvert==7.8.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 198))\n",
            "  Using cached nbconvert-7.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting nbformat==5.9.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 199))\n",
            "  Using cached nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting nest-asyncio==1.5.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 200))\n",
            "  Using cached nest_asyncio-1.5.7-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting networkx==3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 201))\n",
            "  Using cached networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting nltk==3.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 202))\n",
            "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting notebook-shim==0.2.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 203))\n",
            "  Using cached notebook_shim-0.2.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting notebook==7.0.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 204))\n",
            "  Using cached notebook-7.0.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numba==0.58.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 205))\n",
            "  Using cached numba-0.58.1.tar.gz (2.6 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sherlock_text_path =path+\"data/sherlock_holmes_1.txt\"\n",
        "def read_text_file(filename):\n",
        "  file = open(filename, \"r\", encoding=\"utf-8\")\n",
        "  return file.read()\n",
        "sherlock_text = read_text_file(sherlock_text_path)\n",
        "print(sherlock_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87c3BGs9bN_G",
        "outputId": "2a4f3a82-e08d-4617-f296-3a3f8259757b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
            "mention her under any other name. In his eyes she eclipses and\n",
            "predominates the whole of her sex. It was not that he felt any emotion\n",
            "akin to love for Irene Adler. All emotions, and that one particularly,\n",
            "were abhorrent to his cold, precise but admirably balanced mind. He\n",
            "was, I take it, the most perfect reasoning and observing machine that\n",
            "the world has seen, but as a lover he would have placed himself in a\n",
            "false position. He never spoke of the softer passions, save with a gibe\n",
            "and a sneer. They were admirable things for the observerexcellent for\n",
            "drawing the veil from mens motives and actions. But for the trained\n",
            "reasoner to admit such intrusions into his own delicate and finely\n",
            "adjusted temperament was to introduce a distracting factor which might\n",
            "throw a doubt upon all his mental results. Grit in a sensitive\n",
            "instrument, or a crack in one of his own high-power lenses, would not\n",
            "be more disturbing than a strong emotion in a nature such as his. And\n",
            "yet there was but one woman to him, and that woman was the late Irene\n",
            "Adler, of dubious and questionable memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n",
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "sentences_nltk = tokenizer.tokenize(sherlock_text)\n",
        "print(sentences_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Kk7ib2FpnW",
        "outputId": "2407200f-b536-466b-fb06-fe9e33563c55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\\nmention her under any other name.', 'In his eyes she eclipses and\\npredominates the whole of her sex.', 'It was not that he felt any emotion\\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\\nwas, I take it, the most perfect reasoning and observing machine that\\nthe world has seen, but as a lover he would have placed himself in a\\nfalse position.', 'He never spoke of the softer passions, save with a gibe\\nand a sneer.', 'They were admirable things for the observerexcellent for\\ndrawing the veil from mens motives and actions.', 'But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, would not\\nbe more disturbing than a strong emotion in a nature such as his.', 'And\\nyet there was but one woman to him, and that woman was the late Irene\\nAdler, of dubious and questionable memory.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences_nltk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3WfzXreJl6s",
        "outputId": "55051967-f0fa-41f8-8a23-1b4a71410ca4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.tokenize.word_tokenize(sherlock_text)\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MIMV-F9f7rt",
        "outputId": "f08ea190-8be1-4dd6-943b-1f7340c5a109"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observerexcellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', '', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n",
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mnAxkB_KL6M",
        "outputId": "ba0e0652-7505-4ec1-b61f-2a1821280c8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "31y8-T7sLTxt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sherlock_text)"
      ],
      "metadata": {
        "id": "ossxxLbQLaQB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_spacy = [sentence.text for sentence in doc.sents]\n",
        "print(sentences_spacy)\n",
        "print(len(sentences_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlvTl5KBc6Jf",
        "outputId": "acb8294d-f5d1-4f73-98f0-43d5a851c8f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\\nmention her under any other name.', 'In his eyes she eclipses and\\npredominates the whole of her sex.', 'It was not that he felt any emotion\\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\\nwas, I take it, the most perfect reasoning and observing machine that\\nthe world has seen, but as a lover he would have placed himself in a\\nfalse position.', 'He never spoke of the softer passions, save with a gibe\\nand a sneer.', 'They were admirable things for the observerexcellent for\\ndrawing the veil from mens motives and actions.', 'But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, would not\\nbe more disturbing than a strong emotion in a nature such as his.', 'And\\nyet there was but one woman to him, and that woman was the late Irene\\nAdler, of dubious and questionable memory.']\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_words = [token.text for token in doc]\n",
        "print(spacy_words)\n",
        "print(len(spacy_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kD1MEVthVzr",
        "outputId": "f6c48cef-c0ee-4837-fd90-5f2335f66eed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', '\\n', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', '\\n', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', '\\n', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', '\\n', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', '\\n', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', '\\n', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', '\\n', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', '\\n', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer', '', 'excellent', 'for', '\\n', 'drawing', 'the', 'veil', 'from', 'men', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', '\\n', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', '\\n', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', '\\n', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', '\\n', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', '-', 'power', 'lenses', ',', 'would', 'not', '\\n', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', '\\n', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', '\\n', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n",
            "251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(spacy_words)-set(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qx5w89ZhsmP",
        "outputId": "2af1921e-8206-4a7a-e253-d9d3f2ecf762"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'', '\\n', '-', 'observer', 'power', 's', 'excellent', 'high', '_'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def split_into_sentences_nltk(text):\n",
        "  sentences = tokenizer.tokenize(text)\n",
        "  return sentences\n",
        "def split_into_sentences_spacy(text):\n",
        "  doc = nlp(text)\n",
        "  sentences = [sentence.text for sentence in doc.sents]\n",
        "  return sentences\n",
        "start = time.time()\n",
        "split_into_sentences_nltk(sherlock_text)\n",
        "print(f\"NLTK: {time.time() - start} s\")\n",
        "start = time.time()\n",
        "split_into_sentences_spacy(sherlock_text)\n",
        "print(f\"spaCy: {time.time() - start} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDmgeqo0dXsq",
        "outputId": "7e850e2e-f2d7-4aa1-aa28-3c884003260e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK: 0.00046253204345703125 s\n",
            "spaCy: 0.045632123947143555 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "%run -i '/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/util/lang_utils.ipynb'\n",
        "%run -i '/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/util/file_utils.ipynb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpfgPEeVpFVY",
        "outputId": "12b5334b-dfa4-4766-eb23-e3169bb0b306"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_spacy(text, model):\n",
        "  doc = model(text)\n",
        "  words = [token.text for token in doc]\n",
        "  pos = [token.pos_ for token in doc]\n",
        "  return list(zip(words, pos))"
      ],
      "metadata": {
        "id": "o5FdW7GrqEez"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_with_pos = pos_tag_spacy(sherlock_text, small_model)\n",
        "print(words_with_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElW9HcqFuX_3",
        "outputId": "93411092-7a56-476b-d824-745e3f587bfb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('To', 'ADP'), ('Sherlock', 'PROPN'), ('Holmes', 'PROPN'), ('she', 'PRON'), ('is', 'AUX'), ('always', 'ADV'), ('_', 'PUNCT'), ('the', 'DET'), ('_', 'PROPN'), ('woman', 'NOUN'), ('.', 'PUNCT'), ('I', 'PRON'), ('have', 'AUX'), ('seldom', 'ADV'), ('heard', 'VERB'), ('him', 'PRON'), ('\\n', 'SPACE'), ('mention', 'VERB'), ('her', 'PRON'), ('under', 'ADP'), ('any', 'DET'), ('other', 'ADJ'), ('name', 'NOUN'), ('.', 'PUNCT'), ('In', 'ADP'), ('his', 'PRON'), ('eyes', 'NOUN'), ('she', 'PRON'), ('eclipses', 'VERB'), ('and', 'CCONJ'), ('\\n', 'SPACE'), ('predominates', 'VERB'), ('the', 'DET'), ('whole', 'NOUN'), ('of', 'ADP'), ('her', 'PRON'), ('sex', 'NOUN'), ('.', 'PUNCT'), ('It', 'PRON'), ('was', 'AUX'), ('not', 'PART'), ('that', 'SCONJ'), ('he', 'PRON'), ('felt', 'VERB'), ('any', 'DET'), ('emotion', 'NOUN'), ('\\n', 'SPACE'), ('akin', 'ADJ'), ('to', 'PART'), ('love', 'VERB'), ('for', 'ADP'), ('Irene', 'PROPN'), ('Adler', 'PROPN'), ('.', 'PUNCT'), ('All', 'DET'), ('emotions', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('that', 'DET'), ('one', 'NUM'), ('particularly', 'ADV'), (',', 'PUNCT'), ('\\n', 'SPACE'), ('were', 'AUX'), ('abhorrent', 'ADJ'), ('to', 'ADP'), ('his', 'PRON'), ('cold', 'ADJ'), (',', 'PUNCT'), ('precise', 'ADJ'), ('but', 'CCONJ'), ('admirably', 'ADV'), ('balanced', 'ADJ'), ('mind', 'NOUN'), ('.', 'PUNCT'), ('He', 'PRON'), ('\\n', 'SPACE'), ('was', 'AUX'), (',', 'PUNCT'), ('I', 'PRON'), ('take', 'VERB'), ('it', 'PRON'), (',', 'PUNCT'), ('the', 'DET'), ('most', 'ADV'), ('perfect', 'ADJ'), ('reasoning', 'NOUN'), ('and', 'CCONJ'), ('observing', 'VERB'), ('machine', 'NOUN'), ('that', 'PRON'), ('\\n', 'SPACE'), ('the', 'DET'), ('world', 'NOUN'), ('has', 'AUX'), ('seen', 'VERB'), (',', 'PUNCT'), ('but', 'CCONJ'), ('as', 'ADP'), ('a', 'DET'), ('lover', 'NOUN'), ('he', 'PRON'), ('would', 'AUX'), ('have', 'AUX'), ('placed', 'VERB'), ('himself', 'PRON'), ('in', 'ADP'), ('a', 'DET'), ('\\n', 'SPACE'), ('false', 'ADJ'), ('position', 'NOUN'), ('.', 'PUNCT'), ('He', 'PRON'), ('never', 'ADV'), ('spoke', 'VERB'), ('of', 'ADP'), ('the', 'DET'), ('softer', 'ADJ'), ('passions', 'NOUN'), (',', 'PUNCT'), ('save', 'VERB'), ('with', 'ADP'), ('a', 'DET'), ('gibe', 'NOUN'), ('\\n', 'SPACE'), ('and', 'CCONJ'), ('a', 'DET'), ('sneer', 'NOUN'), ('.', 'PUNCT'), ('They', 'PRON'), ('were', 'AUX'), ('admirable', 'ADJ'), ('things', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('observer', 'NOUN'), ('', 'PUNCT'), ('excellent', 'ADJ'), ('for', 'ADP'), ('\\n', 'SPACE'), ('drawing', 'VERB'), ('the', 'DET'), ('veil', 'NOUN'), ('from', 'ADP'), ('men', 'NOUN'), ('s', 'PART'), ('motives', 'NOUN'), ('and', 'CCONJ'), ('actions', 'NOUN'), ('.', 'PUNCT'), ('But', 'CCONJ'), ('for', 'SCONJ'), ('the', 'DET'), ('trained', 'VERB'), ('\\n', 'SPACE'), ('reasoner', 'NOUN'), ('to', 'PART'), ('admit', 'VERB'), ('such', 'ADJ'), ('intrusions', 'NOUN'), ('into', 'ADP'), ('his', 'PRON'), ('own', 'ADJ'), ('delicate', 'ADJ'), ('and', 'CCONJ'), ('finely', 'ADV'), ('\\n', 'SPACE'), ('adjusted', 'VERB'), ('temperament', 'NOUN'), ('was', 'AUX'), ('to', 'PART'), ('introduce', 'VERB'), ('a', 'DET'), ('distracting', 'NOUN'), ('factor', 'NOUN'), ('which', 'PRON'), ('might', 'AUX'), ('\\n', 'SPACE'), ('throw', 'VERB'), ('a', 'DET'), ('doubt', 'NOUN'), ('upon', 'SCONJ'), ('all', 'DET'), ('his', 'PRON'), ('mental', 'ADJ'), ('results', 'NOUN'), ('.', 'PUNCT'), ('Grit', 'NOUN'), ('in', 'ADP'), ('a', 'DET'), ('sensitive', 'ADJ'), ('\\n', 'SPACE'), ('instrument', 'NOUN'), (',', 'PUNCT'), ('or', 'CCONJ'), ('a', 'DET'), ('crack', 'NOUN'), ('in', 'ADP'), ('one', 'NUM'), ('of', 'ADP'), ('his', 'PRON'), ('own', 'ADJ'), ('high', 'ADJ'), ('-', 'PUNCT'), ('power', 'NOUN'), ('lenses', 'NOUN'), (',', 'PUNCT'), ('would', 'AUX'), ('not', 'PART'), ('\\n', 'SPACE'), ('be', 'AUX'), ('more', 'ADV'), ('disturbing', 'ADJ'), ('than', 'ADP'), ('a', 'DET'), ('strong', 'ADJ'), ('emotion', 'NOUN'), ('in', 'ADP'), ('a', 'DET'), ('nature', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('his', 'PRON'), ('.', 'PUNCT'), ('And', 'CCONJ'), ('\\n', 'SPACE'), ('yet', 'ADV'), ('there', 'PRON'), ('was', 'VERB'), ('but', 'CCONJ'), ('one', 'NUM'), ('woman', 'NOUN'), ('to', 'ADP'), ('him', 'PRON'), (',', 'PUNCT'), ('and', 'CCONJ'), ('that', 'DET'), ('woman', 'NOUN'), ('was', 'AUX'), ('the', 'DET'), ('late', 'ADJ'), ('Irene', 'PROPN'), ('\\n', 'SPACE'), ('Adler', 'PROPN'), (',', 'PUNCT'), ('of', 'ADP'), ('dubious', 'ADJ'), ('and', 'CCONJ'), ('questionable', 'ADJ'), ('memory', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_nltk(text):\n",
        "  words = word_tokenize_nltk(text)\n",
        "  words_with_pos = nltk.pos_tag(words)\n",
        "  return words_with_pos"
      ],
      "metadata": {
        "id": "J5rnWQk9vFzY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEQXJLVsvpL-",
        "outputId": "ba198775-863a-44e2-a676-269d1b9a9aa3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('To', 'TO'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), ('she', 'PRP'), ('is', 'VBZ'), ('always', 'RB'), ('_the_', 'JJ'), ('woman', 'NN'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('seldom', 'VBN'), ('heard', 'RB'), ('him', 'PRP'), ('mention', 'VB'), ('her', 'PRP'), ('under', 'IN'), ('any', 'DT'), ('other', 'JJ'), ('name', 'NN'), ('.', '.'), ('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('felt', 'VBD'), ('any', 'DT'), ('emotion', 'NN'), ('akin', 'NN'), ('to', 'TO'), ('love', 'VB'), ('for', 'IN'), ('Irene', 'NNP'), ('Adler', 'NNP'), ('.', '.'), ('All', 'DT'), ('emotions', 'NNS'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('one', 'CD'), ('particularly', 'RB'), (',', ','), ('were', 'VBD'), ('abhorrent', 'JJ'), ('to', 'TO'), ('his', 'PRP$'), ('cold', 'NN'), (',', ','), ('precise', 'NN'), ('but', 'CC'), ('admirably', 'RB'), ('balanced', 'VBD'), ('mind', 'NN'), ('.', '.'), ('He', 'PRP'), ('was', 'VBD'), (',', ','), ('I', 'PRP'), ('take', 'VBP'), ('it', 'PRP'), (',', ','), ('the', 'DT'), ('most', 'RBS'), ('perfect', 'JJ'), ('reasoning', 'NN'), ('and', 'CC'), ('observing', 'VBG'), ('machine', 'NN'), ('that', 'IN'), ('the', 'DT'), ('world', 'NN'), ('has', 'VBZ'), ('seen', 'VBN'), (',', ','), ('but', 'CC'), ('as', 'IN'), ('a', 'DT'), ('lover', 'NN'), ('he', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('placed', 'VBN'), ('himself', 'PRP'), ('in', 'IN'), ('a', 'DT'), ('false', 'JJ'), ('position', 'NN'), ('.', '.'), ('He', 'PRP'), ('never', 'RB'), ('spoke', 'VBD'), ('of', 'IN'), ('the', 'DT'), ('softer', 'JJR'), ('passions', 'NNS'), (',', ','), ('save', 'VBP'), ('with', 'IN'), ('a', 'DT'), ('gibe', 'NN'), ('and', 'CC'), ('a', 'DT'), ('sneer', 'NN'), ('.', '.'), ('They', 'PRP'), ('were', 'VBD'), ('admirable', 'JJ'), ('things', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('observerexcellent', 'NN'), ('for', 'IN'), ('drawing', 'VBG'), ('the', 'DT'), ('veil', 'NN'), ('from', 'IN'), ('men', 'NNS'), ('', 'VBP'), ('s', 'JJ'), ('motives', 'NNS'), ('and', 'CC'), ('actions', 'NNS'), ('.', '.'), ('But', 'CC'), ('for', 'IN'), ('the', 'DT'), ('trained', 'JJ'), ('reasoner', 'NN'), ('to', 'TO'), ('admit', 'VB'), ('such', 'JJ'), ('intrusions', 'NNS'), ('into', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('delicate', 'NN'), ('and', 'CC'), ('finely', 'RB'), ('adjusted', 'VBD'), ('temperament', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('introduce', 'VB'), ('a', 'DT'), ('distracting', 'NN'), ('factor', 'NN'), ('which', 'WDT'), ('might', 'MD'), ('throw', 'VB'), ('a', 'DT'), ('doubt', 'NN'), ('upon', 'IN'), ('all', 'PDT'), ('his', 'PRP$'), ('mental', 'JJ'), ('results', 'NNS'), ('.', '.'), ('Grit', 'NNP'), ('in', 'IN'), ('a', 'DT'), ('sensitive', 'JJ'), ('instrument', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('crack', 'NN'), ('in', 'IN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('high-power', 'NN'), ('lenses', 'NNS'), (',', ','), ('would', 'MD'), ('not', 'RB'), ('be', 'VB'), ('more', 'RBR'), ('disturbing', 'JJ'), ('than', 'IN'), ('a', 'DT'), ('strong', 'JJ'), ('emotion', 'NN'), ('in', 'IN'), ('a', 'DT'), ('nature', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('his', 'PRP$'), ('.', '.'), ('And', 'CC'), ('yet', 'RB'), ('there', 'EX'), ('was', 'VBD'), ('but', 'CC'), ('one', 'CD'), ('woman', 'NN'), ('to', 'TO'), ('him', 'PRP'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('woman', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('late', 'JJ'), ('Irene', 'NNP'), ('Adler', 'NNP'), (',', ','), ('of', 'IN'), ('dubious', 'JJ'), ('and', 'CC'), ('questionable', 'JJ'), ('memory', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_with_pos = pos_tag_nltk(sherlock_text)\n",
        "print(words_with_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4blrradwFzO",
        "outputId": "d9d73ee5-9d26-4ba7-a811-3bed913c7e36"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('To', 'TO'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), ('she', 'PRP'), ('is', 'VBZ'), ('always', 'RB'), ('_the_', 'JJ'), ('woman', 'NN'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('seldom', 'VBN'), ('heard', 'RB'), ('him', 'PRP'), ('mention', 'VB'), ('her', 'PRP'), ('under', 'IN'), ('any', 'DT'), ('other', 'JJ'), ('name', 'NN'), ('.', '.'), ('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('felt', 'VBD'), ('any', 'DT'), ('emotion', 'NN'), ('akin', 'NN'), ('to', 'TO'), ('love', 'VB'), ('for', 'IN'), ('Irene', 'NNP'), ('Adler', 'NNP'), ('.', '.'), ('All', 'DT'), ('emotions', 'NNS'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('one', 'CD'), ('particularly', 'RB'), (',', ','), ('were', 'VBD'), ('abhorrent', 'JJ'), ('to', 'TO'), ('his', 'PRP$'), ('cold', 'NN'), (',', ','), ('precise', 'NN'), ('but', 'CC'), ('admirably', 'RB'), ('balanced', 'VBD'), ('mind', 'NN'), ('.', '.'), ('He', 'PRP'), ('was', 'VBD'), (',', ','), ('I', 'PRP'), ('take', 'VBP'), ('it', 'PRP'), (',', ','), ('the', 'DT'), ('most', 'RBS'), ('perfect', 'JJ'), ('reasoning', 'NN'), ('and', 'CC'), ('observing', 'VBG'), ('machine', 'NN'), ('that', 'IN'), ('the', 'DT'), ('world', 'NN'), ('has', 'VBZ'), ('seen', 'VBN'), (',', ','), ('but', 'CC'), ('as', 'IN'), ('a', 'DT'), ('lover', 'NN'), ('he', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('placed', 'VBN'), ('himself', 'PRP'), ('in', 'IN'), ('a', 'DT'), ('false', 'JJ'), ('position', 'NN'), ('.', '.'), ('He', 'PRP'), ('never', 'RB'), ('spoke', 'VBD'), ('of', 'IN'), ('the', 'DT'), ('softer', 'JJR'), ('passions', 'NNS'), (',', ','), ('save', 'VBP'), ('with', 'IN'), ('a', 'DT'), ('gibe', 'NN'), ('and', 'CC'), ('a', 'DT'), ('sneer', 'NN'), ('.', '.'), ('They', 'PRP'), ('were', 'VBD'), ('admirable', 'JJ'), ('things', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('observerexcellent', 'NN'), ('for', 'IN'), ('drawing', 'VBG'), ('the', 'DT'), ('veil', 'NN'), ('from', 'IN'), ('men', 'NNS'), ('', 'VBP'), ('s', 'JJ'), ('motives', 'NNS'), ('and', 'CC'), ('actions', 'NNS'), ('.', '.'), ('But', 'CC'), ('for', 'IN'), ('the', 'DT'), ('trained', 'JJ'), ('reasoner', 'NN'), ('to', 'TO'), ('admit', 'VB'), ('such', 'JJ'), ('intrusions', 'NNS'), ('into', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('delicate', 'NN'), ('and', 'CC'), ('finely', 'RB'), ('adjusted', 'VBD'), ('temperament', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('introduce', 'VB'), ('a', 'DT'), ('distracting', 'NN'), ('factor', 'NN'), ('which', 'WDT'), ('might', 'MD'), ('throw', 'VB'), ('a', 'DT'), ('doubt', 'NN'), ('upon', 'IN'), ('all', 'PDT'), ('his', 'PRP$'), ('mental', 'JJ'), ('results', 'NNS'), ('.', '.'), ('Grit', 'NNP'), ('in', 'IN'), ('a', 'DT'), ('sensitive', 'JJ'), ('instrument', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('crack', 'NN'), ('in', 'IN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('high-power', 'NN'), ('lenses', 'NNS'), (',', ','), ('would', 'MD'), ('not', 'RB'), ('be', 'VB'), ('more', 'RBR'), ('disturbing', 'JJ'), ('than', 'IN'), ('a', 'DT'), ('strong', 'JJ'), ('emotion', 'NN'), ('in', 'IN'), ('a', 'DT'), ('nature', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('his', 'PRP$'), ('.', '.'), ('And', 'CC'), ('yet', 'RB'), ('there', 'EX'), ('was', 'VBD'), ('but', 'CC'), ('one', 'CD'), ('woman', 'NN'), ('to', 'TO'), ('him', 'PRP'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('woman', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('late', 'JJ'), ('Irene', 'NNP'), ('Adler', 'NNP'), (',', ','), ('of', 'IN'), ('dubious', 'JJ'), ('and', 'CC'), ('questionable', 'JJ'), ('memory', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing STOP Words**\n"
      ],
      "metadata": {
        "id": "KHaSpBGR3rOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewPzr2jy31Jf",
        "outputId": "464ad901-957d-432f-bf93-b70e981e7d7b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNU9IBAG361Q",
        "outputId": "5c6b186f-b371-491d-a358-718e61e70e3e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize_nltk(sherlock_text)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8qOIPfc4CYl",
        "outputId": "f75c0fde-c027-4b6d-9568-fe38292f5893"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in words if word not in stopwords.words('english')]\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwVUcBWq4N8s",
        "outputId": "fe3e8b42-c8a1-43f2-db0f-440d63872b88"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129\n"
          ]
        }
      ]
    }
  ]
}