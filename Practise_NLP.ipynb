{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR+sUotA5mA+O/EMApyDbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gowtham-Pentela/NLP/blob/main/Practise_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I-WNmXR4_S9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e935bb8f-7273-4f4a-fcf4-d6975de1985b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting poetry\n",
            "  Downloading poetry-2.3.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting build<2.0.0,>=1.2.1 (from poetry)\n",
            "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (0.14.4)\n",
            "Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n",
            "  Downloading cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dulwich<2,>=0.25.0 (from poetry)\n",
            "  Downloading dulwich-1.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.21.2)\n",
            "Collecting findpython<0.8.0,>=0.6.2 (from poetry)\n",
            "  Downloading findpython-0.7.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting installer<0.8.0,>=0.7.0 (from poetry)\n",
            "  Downloading installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
            "Requirement already satisfied: keyring<26.0.0,>=25.1.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (25.7.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.12/dist-packages (from poetry) (26.0)\n",
            "Collecting pbs-installer>=2025.6.10 (from pbs-installer[download,install]>=2025.6.10->poetry)\n",
            "  Downloading pbs_installer-2026.2.11-py3-none-any.whl.metadata (991 bytes)\n",
            "Collecting pkginfo<2.0,>=1.12 (from poetry)\n",
            "  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (4.9.2)\n",
            "Collecting poetry-core==2.3.1 (from poetry)\n",
            "  Downloading poetry_core-2.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting pyproject-hooks<2.0.0,>=1.0.0 (from poetry)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.26 in /usr/local/lib/python3.12/dist-packages (from poetry) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.0.0)\n",
            "Requirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.12/dist-packages (from poetry) (1.5.4)\n",
            "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /usr/local/lib/python3.12/dist-packages (from poetry) (0.13.3)\n",
            "Collecting trove-classifiers>=2022.5.19 (from poetry)\n",
            "  Downloading trove_classifiers-2026.1.14.14-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting virtualenv>=20.26.6 (from poetry)\n",
            "  Downloading virtualenv-20.39.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (1.1.2)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (3.24.2)\n",
            "Collecting crashtest<0.5.0,>=0.4.1 (from cleo<3.0.0,>=2.1.0->poetry)\n",
            "  Downloading crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from dulwich<2,>=0.25.0->poetry) (2.5.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.5.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (4.4.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (6.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]>=2025.6.10->poetry) (0.28.1)\n",
            "Requirement already satisfied: zstandard>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pbs-installer[download,install]>=2025.6.10->poetry) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.26->poetry) (2026.1.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.26.6->poetry)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (0.16.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (2.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]>=2025.6.10->poetry) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (3.0)\n",
            "Downloading poetry-2.3.2-py3-none-any.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading poetry_core-2.3.1-py3-none-any.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.9/340.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
            "Downloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-1.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading findpython-0.7.1-py3-none-any.whl (21 kB)\n",
            "Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbs_installer-2026.2.11-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading trove_classifiers-2026.1.14.14-py3-none-any.whl (14 kB)\n",
            "Downloading virtualenv-20.39.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trove-classifiers, distlib, virtualenv, rapidfuzz, pyproject-hooks, poetry-core, pkginfo, pbs-installer, installer, findpython, dulwich, crashtest, cleo, build, poetry\n",
            "Successfully installed build-1.4.0 cleo-2.1.0 crashtest-0.4.1 distlib-0.4.0 dulwich-1.1.0 findpython-0.7.1 installer-0.7.0 pbs-installer-2026.2.11 pkginfo-1.12.1.2 poetry-2.3.2 poetry-core-2.3.1 pyproject-hooks-1.2.0 rapidfuzz-3.14.3 trove-classifiers-2026.1.14.14 virtualenv-20.39.0\n",
            "Cloning into 'Python-Natural-Language-Processing-Cookbook-Second-Edition'...\n",
            "remote: Enumerating objects: 436, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 436 (delta 14), reused 6 (delta 2), pack-reused 406 (from 1)\u001b[K\n",
            "Receiving objects: 100% (436/436), 18.29 MiB | 16.35 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install poetry\n",
        "!git clone \"https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition.git\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/\"\n"
      ],
      "metadata": {
        "id": "4uKuHGL4Ex7B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run -i '/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/util/file_utils.ipynb'\n",
        "\n"
      ],
      "metadata": {
        "id": "9t-d9NWJbMDq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBIvJcMdcQ3l",
        "outputId": "f3b5256c-0a84-42ff-b454-ee147c4aba96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring appnope: markers 'python_version >= \"3.9\" and python_version < \"4.0\" and platform_system == \"Darwin\" or python_version >= \"3.9\" and python_version < \"4.0\" and sys_platform == \"darwin\"' don't match your environment\n",
            "Ignoring exceptiongroup: markers 'python_version >= \"3.9\" and python_version < \"3.11\"' don't match your environment\n",
            "Ignoring importlib-metadata: markers 'python_version >= \"3.9\" and python_version < \"3.10\"' don't match your environment\n",
            "Ignoring importlib-resources: markers 'python_version >= \"3.9\" and python_version < \"3.10\"' don't match your environment\n",
            "Ignoring pywin32: markers 'sys_platform == \"win32\" and platform_python_implementation != \"PyPy\" and python_version >= \"3.9\" and python_version < \"4.0\"' don't match your environment\n",
            "Ignoring pywinpty: markers 'python_version >= \"3.9\" and python_version < \"4.0\" and os_name == \"nt\"' don't match your environment\n",
            "Ignoring tomli: markers 'python_version >= \"3.9\" and python_full_version <= \"3.11.0a6\"' don't match your environment\n",
            "Ignoring zipp: markers 'python_version >= \"3.9\" and python_version < \"3.10\"' don't match your environment\n",
            "Collecting accelerate==0.24.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 1))\n",
            "  Downloading accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting aiohttp==3.8.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 2))\n",
            "  Downloading aiohttp-3.8.5.tar.gz (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiosignal==1.3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 3))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting alabaster==0.7.13 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 4))\n",
            "  Downloading alabaster-0.7.13-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting annotated-types==0.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 5))\n",
            "  Downloading annotated_types-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting anyio==4.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 6))\n",
            "  Downloading anyio-4.0.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting argon2-cffi-bindings==21.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 8))\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting argon2-cffi==23.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 9))\n",
            "  Downloading argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting arrow==1.2.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 10))\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting astatine==0.3.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 11))\n",
            "  Downloading astatine-0.3.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting astor==0.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 12))\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting astpretty==3.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 13))\n",
            "  Downloading astpretty-3.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting astroid==3.0.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 14))\n",
            "  Downloading astroid-3.0.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting asttokens==2.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 15))\n",
            "  Downloading asttokens-2.4.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting async-lru==2.0.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 16))\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting async-timeout==4.0.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 17))\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting attrs==23.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 18))\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autoflake==1.7.8 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 19))\n",
            "  Downloading autoflake-1.7.8-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting babel==2.12.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 20))\n",
            "  Downloading Babel-2.12.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 21)) (0.2.0)\n",
            "Collecting bandit==1.7.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 22))\n",
            "  Downloading bandit-1.7.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting beautifulsoup4==4.12.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 23))\n",
            "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting bertopic==0.16.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 24))\n",
            "  Downloading bertopic-0.16.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting black==23.11.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 25))\n",
            "  Downloading black-23.11.0-py3-none-any.whl.metadata (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bleach==6.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 26))\n",
            "  Downloading bleach-6.0.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting blis==0.7.10 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 27))\n",
            "  Downloading blis-0.7.10.tar.gz (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cachetools==5.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 28))\n",
            "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting catalogue==2.0.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 29))\n",
            "  Downloading catalogue-2.0.9-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting certifi==2023.7.22 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 30))\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting cffi==1.15.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 31))\n",
            "  Downloading cffi-1.15.1.tar.gz (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.5/508.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet==5.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 32)) (5.2.0)\n",
            "Collecting charset-normalizer==3.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 33))\n",
            "  Downloading charset_normalizer-3.2.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting click==8.1.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 34))\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cmake==3.27.4.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 35))\n",
            "  Downloading cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting cognitive-complexity==1.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 36))\n",
            "  Downloading cognitive_complexity-1.3.0.tar.gz (5.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama==0.4.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 37))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting comm==0.1.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 38))\n",
            "  Downloading comm-0.1.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting confection==0.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 39))\n",
            "  Downloading confection-0.1.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting contextualized-topic-models==2.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 40))\n",
            "  Downloading contextualized_topic_models-2.5.0-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Collecting contourpy==1.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 41))\n",
            "  Downloading contourpy-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting coverage==7.3.2 (from coverage[toml]==7.3.2->-r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 42))\n",
            "  Downloading coverage-7.3.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 43)) (0.12.1)\n",
            "Collecting cymem==2.0.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 44))\n",
            "  Downloading cymem-2.0.7.tar.gz (9.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cython==0.29.36 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 45))\n",
            "  Downloading Cython-0.29.36-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting darglint==1.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 46))\n",
            "  Downloading darglint-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dataclasses-json==0.5.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 47))\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting datasets==2.14.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 48))\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting debugpy==1.6.7.post1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 49))\n",
            "  Downloading debugpy-1.6.7.post1-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting decorator==5.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 50))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: defusedxml==0.7.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 51)) (0.7.1)\n",
            "Collecting dill==0.3.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 52))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting distlib==0.3.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 53))\n",
            "  Downloading distlib-0.3.7-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 54)) (1.9.0)\n",
            "Collecting dlint==0.14.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 55))\n",
            "  Downloading dlint-0.14.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting doc8==1.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 56))\n",
            "  Downloading doc8-1.1.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting docformatter==1.7.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 57))\n",
            "  Downloading docformatter-1.7.5-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting docutils==0.19 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 58))\n",
            "  Downloading docutils-0.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting domdf-python-tools==3.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 59))\n",
            "  Downloading domdf_python_tools-3.7.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting eradicate==2.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 60))\n",
            "  Downloading eradicate-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting evaluate==0.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 61))\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting executing==1.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 63))\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting fastapi==0.103.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 64))\n",
            "  Downloading fastapi-0.103.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting fastjsonschema==2.18.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 65))\n",
            "  Downloading fastjsonschema-2.18.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting filelock==3.12.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 66))\n",
            "  Downloading filelock-3.12.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting flake8-2020==1.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 67))\n",
            "  Downloading flake8_2020-1.8.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting flake8-aaa==0.17.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 68))\n",
            "  Downloading flake8_aaa-0.17.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting flake8-annotations-complexity==0.0.8 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 69))\n",
            "  Downloading flake8_annotations_complexity-0.0.8-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-annotations-coverage==0.0.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 70))\n",
            "  Downloading flake8_annotations_coverage-0.0.6-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting flake8-annotations==3.0.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 71))\n",
            "  Downloading flake8_annotations-3.0.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-bandit==4.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 72))\n",
            "  Downloading flake8_bandit-4.1.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-black==0.3.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 73))\n",
            "  Downloading flake8_black-0.3.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-blind-except==0.2.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 74))\n",
            "  Downloading flake8-blind-except-0.2.1.tar.gz (3.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-breakpoint==1.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 75))\n",
            "  Downloading flake8_breakpoint-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-broken-line==0.6.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 76))\n",
            "  Downloading flake8_broken_line-0.6.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting flake8-bugbear==23.3.12 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 77))\n",
            "  Downloading flake8_bugbear-23.3.12-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting flake8-builtins==1.5.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 78))\n",
            "  Downloading flake8_builtins-1.5.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting flake8-class-attributes-order==0.1.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 79))\n",
            "  Downloading flake8_class_attributes_order-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting flake8-coding==1.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 80))\n",
            "  Downloading flake8_coding-1.3.2-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-cognitive-complexity==0.1.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 81))\n",
            "  Downloading flake8_cognitive_complexity-0.1.0.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-comments==0.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 82))\n",
            "  Downloading flake8_comments-0.1.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting flake8-comprehensions==3.14.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 83))\n",
            "  Downloading flake8_comprehensions-3.14.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-debugger==4.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 84))\n",
            "  Downloading flake8_debugger-4.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting flake8-django==1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 85))\n",
            "  Downloading flake8_django-1.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-docstrings==1.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 86))\n",
            "  Downloading flake8_docstrings-1.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting flake8-encodings==0.5.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 87))\n",
            "  Downloading flake8_encodings-0.5.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting flake8-eradicate==1.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 88))\n",
            "  Downloading flake8_eradicate-1.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flake8-executable==2.1.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 89))\n",
            "  Downloading flake8_executable-2.1.3-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting flake8-expression-complexity==0.0.11 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 90))\n",
            "  Downloading flake8_expression_complexity-0.0.11-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting flake8-fastapi==0.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 91))\n",
            "  Downloading flake8_fastapi-0.7.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting flake8-fixme==1.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 92))\n",
            "  Downloading flake8_fixme-1.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting flake8-functions-names==0.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 93))\n",
            "  Downloading flake8_functions_names-0.4.0.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-functions==0.0.8 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 94))\n",
            "  Downloading flake8_functions-0.0.8-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-future-annotations==0.0.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 95))\n",
            "  Downloading flake8_future_annotations-0.0.5-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting flake8-helper==0.2.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 96))\n",
            "  Downloading flake8_helper-0.2.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting flake8-isort==6.1.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 97))\n",
            "  Downloading flake8_isort-6.1.1-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting flake8-literal==1.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 98))\n",
            "  Downloading flake8_literal-1.3.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting flake8-logging-format==0.9.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 99))\n",
            "  Downloading flake8-logging-format-0.9.0.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-markdown==0.5.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 100))\n",
            "  Downloading flake8_markdown-0.5.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting flake8-mutable==1.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 101))\n",
            "  Downloading flake8-mutable-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-no-pep420==2.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 102))\n",
            "  Downloading flake8_no_pep420-2.7.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting flake8-noqa==1.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 103))\n",
            "  Downloading flake8_noqa-1.3.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting flake8-pie==0.16.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 104))\n",
            "  Downloading flake8_pie-0.16.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-plugin-utils==1.3.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 105))\n",
            "  Downloading flake8_plugin_utils-1.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting flake8-pyi==22.11.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 106))\n",
            "  Downloading flake8_pyi-22.11.0-py37-none-any.whl.metadata (12 kB)\n",
            "Collecting flake8-pylint==0.2.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 107))\n",
            "  Downloading flake8_pylint-0.2.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting flake8-pytest-style==1.7.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 108))\n",
            "  Downloading flake8_pytest_style-1.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting flake8-quotes==3.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 109))\n",
            "  Downloading flake8-quotes-3.3.2.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-rst-docstrings==0.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 110))\n",
            "  Downloading flake8_rst_docstrings-0.3.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting flake8-secure-coding-standard==1.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 111))\n",
            "  Downloading flake8_secure_coding_standard-1.4.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting flake8-simplify==0.21.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 112))\n",
            "  Downloading flake8_simplify-0.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting flake8-string-format==0.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 113))\n",
            "  Downloading flake8_string_format-0.3.0-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting flake8-tidy-imports==4.10.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 114))\n",
            "  Downloading flake8_tidy_imports-4.10.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting flake8-typing-imports==1.15.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 115))\n",
            "  Downloading flake8_typing_imports-1.15.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flake8-use-fstring==1.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 116))\n",
            "  Downloading flake8-use-fstring-1.4.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8-use-pathlib==0.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 117))\n",
            "  Downloading flake8_use_pathlib-0.3.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting flake8-useless-assert==0.4.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 118))\n",
            "  Downloading flake8_useless_assert-0.4.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting flake8-variables-names==0.0.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 119))\n",
            "  Downloading flake8_variables_names-0.0.6-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting flake8-warnings==0.4.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 120))\n",
            "  Downloading flake8_warnings-0.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flake8==5.0.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 121))\n",
            "  Downloading flake8-5.0.4-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting fonttools==4.43.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 122))\n",
            "  Downloading fonttools-4.43.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fqdn==1.5.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 123)) (1.5.1)\n",
            "Collecting frozenlist==1.4.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 124))\n",
            "  Downloading frozenlist-1.4.0.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fsspec==2023.6.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 125))\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting funcy==2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 127))\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting gensim==4.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 128))\n",
            "  Downloading gensim-4.2.0.tar.gz (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitdb==4.0.11 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 129))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting gitpython==3.1.40 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 130))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting greenlet==2.0.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 131))\n",
            "  Downloading greenlet-2.0.2.tar.gz (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h11==0.14.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 132))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting hdbscan==0.8.33 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 133))\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpcore==1.0.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 134))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting httpx==0.27.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 135))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting huggingface-hub==0.16.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 136))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hypothesis==6.91.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 137))\n",
            "  Downloading hypothesis-6.91.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting hypothesmith==0.1.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 138))\n",
            "  Downloading hypothesmith-0.1.9-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting idna==3.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 139))\n",
            "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: imagesize==1.4.1 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 140)) (1.4.1)\n",
            "Collecting iniconfig==2.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 143))\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ipykernel==6.25.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 144))\n",
            "  Downloading ipykernel-6.25.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 145)) (0.2.0)\n",
            "Collecting ipython==8.10.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 146))\n",
            "  Downloading ipython-8.10.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting ipywidgets==7.5.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 147))\n",
            "  Downloading ipywidgets-7.5.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: isoduration==20.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 148)) (20.11.0)\n",
            "Collecting isort==5.12.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 149))\n",
            "  Downloading isort-5.12.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting jedi==0.19.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 150))\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jinja2==3.1.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 151))\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting joblib==1.3.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 152))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting json5==0.9.14 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 153))\n",
            "  Downloading json5-0.9.14-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonpointer==2.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 154))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting jsonschema-specifications==2023.7.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 155))\n",
            "  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting jsonschema==4.19.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 156))\n",
            "  Downloading jsonschema-4.19.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jupyter-client==8.3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 158))\n",
            "  Downloading jupyter_client-8.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: jupyter-console==6.6.3 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 159)) (6.6.3)\n",
            "Collecting jupyter-core==5.3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 160))\n",
            "  Downloading jupyter_core-5.3.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting jupyter-events==0.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 161))\n",
            "  Downloading jupyter_events-0.7.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jupyter-lsp==2.2.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 162))\n",
            "  Downloading jupyter_lsp-2.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server-terminals==0.4.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 163))\n",
            "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jupyter-server==2.7.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 164))\n",
            "  Downloading jupyter_server-2.7.3-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting jupyter==1.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 165))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting jupyterlab-flake8==0.7.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 166))\n",
            "  Downloading jupyterlab_flake8-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyterlab-pygments==0.2.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 167))\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting jupyterlab-server==2.24.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 168))\n",
            "  Downloading jupyterlab_server-2.24.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyterlab==4.0.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 169))\n",
            "  Downloading jupyterlab-4.0.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting kiwisolver==1.4.5 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 170))\n",
            "  Downloading kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting langchain==0.0.284 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 171))\n",
            "  Downloading langchain-0.0.284-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langcodes==3.3.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 172))\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langdetect==1.0.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 173))\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langsmith==0.0.33 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 174))\n",
            "  Downloading langsmith-0.0.33-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting lark-parser==0.12.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 175))\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting levenshtein==0.22.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 176))\n",
            "  Downloading Levenshtein-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting libcst==0.4.10 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 177))\n",
            "  Downloading libcst-0.4.10.tar.gz (752 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.7/752.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lit==16.0.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 178))\n",
            "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama-index==0.7.24.post1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 179))\n",
            "  Downloading llama_index-0.7.24.post1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting llvmlite==0.41.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 180))\n",
            "  Downloading llvmlite-0.41.1.tar.gz (146 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting markdown-it-py==3.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 181))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting markupsafe==2.1.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 182))\n",
            "  Downloading MarkupSafe-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting marshmallow-enum==1.5.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 183))\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting marshmallow==3.20.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 184))\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting matplotlib-inline==0.1.6 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 185))\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting matplotlib==3.8.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 186))\n",
            "  Downloading matplotlib-3.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting mccabe==0.7.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 187))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 188)) (0.1.2)\n",
            "Collecting mistune==3.0.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 189))\n",
            "  Downloading mistune-3.0.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 190)) (1.3.0)\n",
            "Collecting mr-proper==0.0.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 191))\n",
            "  Downloading mr_proper-0.0.7-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting multidict==6.0.4 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 192))\n",
            "  Downloading multidict-6.0.4.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting multiprocess==0.70.15 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 193))\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting murmurhash==1.0.9 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 194))\n",
            "  Downloading murmurhash-1.0.9.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mypy-extensions==1.0.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 195))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: natsort==8.4.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 196)) (8.4.0)\n",
            "Collecting nbclient==0.8.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 197))\n",
            "  Downloading nbclient-0.8.0-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting nbconvert==7.8.0 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 198))\n",
            "  Downloading nbconvert-7.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting nbformat==5.9.2 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 199))\n",
            "  Downloading nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting nest-asyncio==1.5.7 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 200))\n",
            "  Downloading nest_asyncio-1.5.7-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting networkx==3.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 201))\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting nltk==3.8.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 202))\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting notebook-shim==0.2.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 203))\n",
            "  Downloading notebook_shim-0.2.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting notebook==7.0.3 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 204))\n",
            "  Downloading notebook-7.0.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numba==0.58.1 (from -r /content/Python-Natural-Language-Processing-Cookbook-Second-Edition/requirements.txt (line 205))\n",
            "  Downloading numba-0.58.1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sherlock_text_path =path+\"data/sherlock_holmes_1.txt\"\n",
        "def read_text_file(filename):\n",
        "  file = open(filename, \"r\", encoding=\"utf-8\")\n",
        "  return file.read()\n",
        "sherlock_text = read_text_file(sherlock_text_path)\n",
        "print(sherlock_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87c3BGs9bN_G",
        "outputId": "558f760a-9ff5-4360-b77b-a7fae29c989f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
            "mention her under any other name. In his eyes she eclipses and\n",
            "predominates the whole of her sex. It was not that he felt any emotion\n",
            "akin to love for Irene Adler. All emotions, and that one particularly,\n",
            "were abhorrent to his cold, precise but admirably balanced mind. He\n",
            "was, I take it, the most perfect reasoning and observing machine that\n",
            "the world has seen, but as a lover he would have placed himself in a\n",
            "false position. He never spoke of the softer passions, save with a gibe\n",
            "and a sneer. They were admirable things for the observer—excellent for\n",
            "drawing the veil from men’s motives and actions. But for the trained\n",
            "reasoner to admit such intrusions into his own delicate and finely\n",
            "adjusted temperament was to introduce a distracting factor which might\n",
            "throw a doubt upon all his mental results. Grit in a sensitive\n",
            "instrument, or a crack in one of his own high-power lenses, would not\n",
            "be more disturbing than a strong emotion in a nature such as his. And\n",
            "yet there was but one woman to him, and that woman was the late Irene\n",
            "Adler, of dubious and questionable memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Add this line to download the missing resource\n",
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "sentences_nltk = tokenizer.tokenize(sherlock_text)\n",
        "print(sentences_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6Kk7ib2FpnW",
        "outputId": "bc38d313-45f1-47d6-c3d1-f97a8e7fded8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\\nmention her under any other name.', 'In his eyes she eclipses and\\npredominates the whole of her sex.', 'It was not that he felt any emotion\\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\\nwas, I take it, the most perfect reasoning and observing machine that\\nthe world has seen, but as a lover he would have placed himself in a\\nfalse position.', 'He never spoke of the softer passions, save with a gibe\\nand a sneer.', 'They were admirable things for the observer—excellent for\\ndrawing the veil from men’s motives and actions.', 'But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, would not\\nbe more disturbing than a strong emotion in a nature such as his.', 'And\\nyet there was but one woman to him, and that woman was the late Irene\\nAdler, of dubious and questionable memory.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences_nltk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3WfzXreJl6s",
        "outputId": "8030c582-4553-4e6a-bcf9-aa6bd7dc5658"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.tokenize.word_tokenize(sherlock_text)\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MIMV-F9f7rt",
        "outputId": "e0a863fd-bbae-4c9b-e924-b6f0a4de2e36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer—excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', '’', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n",
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mnAxkB_KL6M",
        "outputId": "dbb51b7d-f825-4a1d-84ab-ac77390bd06d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "31y8-T7sLTxt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sherlock_text)"
      ],
      "metadata": {
        "id": "ossxxLbQLaQB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_spacy = [sentence.text for sentence in doc.sents]\n",
        "print(sentences_spacy)\n",
        "print(len(sentences_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlvTl5KBc6Jf",
        "outputId": "a139c1ac-d9d7-4d71-c757-15317a498d4a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\\nmention her under any other name.', 'In his eyes she eclipses and\\npredominates the whole of her sex.', 'It was not that he felt any emotion\\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\\nwas, I take it, the most perfect reasoning and observing machine that\\nthe world has seen, but as a lover he would have placed himself in a\\nfalse position.', 'He never spoke of the softer passions, save with a gibe\\nand a sneer.', 'They were admirable things for the observer—excellent for\\ndrawing the veil from men’s motives and actions.', 'But for the trained\\nreasoner to admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a distracting factor which might\\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\\ninstrument, or a crack in one of his own high-power lenses, would not\\nbe more disturbing than a strong emotion in a nature such as his.', 'And\\nyet there was but one woman to him, and that woman was the late Irene\\nAdler, of dubious and questionable memory.']\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_words = [token.text for token in doc]\n",
        "print(spacy_words)\n",
        "print(len(spacy_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kD1MEVthVzr",
        "outputId": "602c1327-2317-4740-c93d-879b9391431a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', '\\n', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', '\\n', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', '\\n', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', '\\n', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', '\\n', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', '\\n', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', '\\n', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', '\\n', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer', '—', 'excellent', 'for', '\\n', 'drawing', 'the', 'veil', 'from', 'men', '’s', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', '\\n', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', '\\n', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', '\\n', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', '\\n', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', '-', 'power', 'lenses', ',', 'would', 'not', '\\n', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', '\\n', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', '\\n', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']\n",
            "251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(spacy_words)-set(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qx5w89ZhsmP",
        "outputId": "2391e4b7-e0ca-4be3-b049-9890754d675a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'power', '\\n', 'excellent', '_', '’s', 'observer', 'high', '-', '—'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def split_into_sentences_nltk(text):\n",
        "  sentences = tokenizer.tokenize(text)\n",
        "  return sentences\n",
        "def split_into_sentences_spacy(text):\n",
        "  doc = nlp(text)\n",
        "  sentences = [sentence.text for sentence in doc.sents]\n",
        "  return sentences\n",
        "start = time.time()\n",
        "split_into_sentences_nltk(sherlock_text)\n",
        "print(f\"NLTK: {time.time() - start} s\")\n",
        "start = time.time()\n",
        "split_into_sentences_spacy(sherlock_text)\n",
        "print(f\"spaCy: {time.time() - start} s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDmgeqo0dXsq",
        "outputId": "f9c6c6f5-9384-4257-8093-a11c7704d107"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK: 0.000986337661743164 s\n",
            "spaCy: 0.07918906211853027 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "%run -i '/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/util/lang_utils.ipynb'\n",
        "%run -i '/content/Python-Natural-Language-Processing-Cookbook-Second-Edition/util/file_utils.ipynb'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpfgPEeVpFVY",
        "outputId": "2af60d12-be25-4dd7-8276-91071ba58085"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_spacy(text, model):\n",
        "  doc = model(text)\n",
        "  words = [token.text for token in doc]\n",
        "  pos = [token.pos_ for token in doc]\n",
        "  return list(zip(words, pos))"
      ],
      "metadata": {
        "id": "o5FdW7GrqEez"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_with_pos = pos_tag_spacy(sherlock_text, small_model)\n",
        "print(words_with_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElW9HcqFuX_3",
        "outputId": "47e37107-093d-47c8-86d5-2b7fae345ea3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('To', 'ADP'), ('Sherlock', 'PROPN'), ('Holmes', 'PROPN'), ('she', 'PRON'), ('is', 'AUX'), ('always', 'ADV'), ('_', 'PUNCT'), ('the', 'DET'), ('_', 'PROPN'), ('woman', 'NOUN'), ('.', 'PUNCT'), ('I', 'PRON'), ('have', 'AUX'), ('seldom', 'ADV'), ('heard', 'VERB'), ('him', 'PRON'), ('\\n', 'SPACE'), ('mention', 'VERB'), ('her', 'PRON'), ('under', 'ADP'), ('any', 'DET'), ('other', 'ADJ'), ('name', 'NOUN'), ('.', 'PUNCT'), ('In', 'ADP'), ('his', 'PRON'), ('eyes', 'NOUN'), ('she', 'PRON'), ('eclipses', 'VERB'), ('and', 'CCONJ'), ('\\n', 'SPACE'), ('predominates', 'VERB'), ('the', 'DET'), ('whole', 'NOUN'), ('of', 'ADP'), ('her', 'PRON'), ('sex', 'NOUN'), ('.', 'PUNCT'), ('It', 'PRON'), ('was', 'AUX'), ('not', 'PART'), ('that', 'SCONJ'), ('he', 'PRON'), ('felt', 'VERB'), ('any', 'DET'), ('emotion', 'NOUN'), ('\\n', 'SPACE'), ('akin', 'ADJ'), ('to', 'PART'), ('love', 'VERB'), ('for', 'ADP'), ('Irene', 'PROPN'), ('Adler', 'PROPN'), ('.', 'PUNCT'), ('All', 'DET'), ('emotions', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('that', 'DET'), ('one', 'NUM'), ('particularly', 'ADV'), (',', 'PUNCT'), ('\\n', 'SPACE'), ('were', 'AUX'), ('abhorrent', 'ADJ'), ('to', 'ADP'), ('his', 'PRON'), ('cold', 'ADJ'), (',', 'PUNCT'), ('precise', 'ADJ'), ('but', 'CCONJ'), ('admirably', 'ADV'), ('balanced', 'ADJ'), ('mind', 'NOUN'), ('.', 'PUNCT'), ('He', 'PRON'), ('\\n', 'SPACE'), ('was', 'AUX'), (',', 'PUNCT'), ('I', 'PRON'), ('take', 'VERB'), ('it', 'PRON'), (',', 'PUNCT'), ('the', 'DET'), ('most', 'ADV'), ('perfect', 'ADJ'), ('reasoning', 'NOUN'), ('and', 'CCONJ'), ('observing', 'VERB'), ('machine', 'NOUN'), ('that', 'PRON'), ('\\n', 'SPACE'), ('the', 'DET'), ('world', 'NOUN'), ('has', 'AUX'), ('seen', 'VERB'), (',', 'PUNCT'), ('but', 'CCONJ'), ('as', 'ADP'), ('a', 'DET'), ('lover', 'NOUN'), ('he', 'PRON'), ('would', 'AUX'), ('have', 'AUX'), ('placed', 'VERB'), ('himself', 'PRON'), ('in', 'ADP'), ('a', 'DET'), ('\\n', 'SPACE'), ('false', 'ADJ'), ('position', 'NOUN'), ('.', 'PUNCT'), ('He', 'PRON'), ('never', 'ADV'), ('spoke', 'VERB'), ('of', 'ADP'), ('the', 'DET'), ('softer', 'ADJ'), ('passions', 'NOUN'), (',', 'PUNCT'), ('save', 'VERB'), ('with', 'ADP'), ('a', 'DET'), ('gibe', 'NOUN'), ('\\n', 'SPACE'), ('and', 'CCONJ'), ('a', 'DET'), ('sneer', 'NOUN'), ('.', 'PUNCT'), ('They', 'PRON'), ('were', 'AUX'), ('admirable', 'ADJ'), ('things', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('observer', 'NOUN'), ('—', 'PUNCT'), ('excellent', 'ADJ'), ('for', 'ADP'), ('\\n', 'SPACE'), ('drawing', 'VERB'), ('the', 'DET'), ('veil', 'NOUN'), ('from', 'ADP'), ('men', 'NOUN'), ('’s', 'PART'), ('motives', 'NOUN'), ('and', 'CCONJ'), ('actions', 'NOUN'), ('.', 'PUNCT'), ('But', 'CCONJ'), ('for', 'SCONJ'), ('the', 'DET'), ('trained', 'VERB'), ('\\n', 'SPACE'), ('reasoner', 'NOUN'), ('to', 'PART'), ('admit', 'VERB'), ('such', 'ADJ'), ('intrusions', 'NOUN'), ('into', 'ADP'), ('his', 'PRON'), ('own', 'ADJ'), ('delicate', 'ADJ'), ('and', 'CCONJ'), ('finely', 'ADV'), ('\\n', 'SPACE'), ('adjusted', 'VERB'), ('temperament', 'NOUN'), ('was', 'AUX'), ('to', 'PART'), ('introduce', 'VERB'), ('a', 'DET'), ('distracting', 'NOUN'), ('factor', 'NOUN'), ('which', 'PRON'), ('might', 'AUX'), ('\\n', 'SPACE'), ('throw', 'VERB'), ('a', 'DET'), ('doubt', 'NOUN'), ('upon', 'SCONJ'), ('all', 'DET'), ('his', 'PRON'), ('mental', 'ADJ'), ('results', 'NOUN'), ('.', 'PUNCT'), ('Grit', 'NOUN'), ('in', 'ADP'), ('a', 'DET'), ('sensitive', 'ADJ'), ('\\n', 'SPACE'), ('instrument', 'NOUN'), (',', 'PUNCT'), ('or', 'CCONJ'), ('a', 'DET'), ('crack', 'NOUN'), ('in', 'ADP'), ('one', 'NUM'), ('of', 'ADP'), ('his', 'PRON'), ('own', 'ADJ'), ('high', 'ADJ'), ('-', 'PUNCT'), ('power', 'NOUN'), ('lenses', 'NOUN'), (',', 'PUNCT'), ('would', 'AUX'), ('not', 'PART'), ('\\n', 'SPACE'), ('be', 'AUX'), ('more', 'ADV'), ('disturbing', 'ADJ'), ('than', 'ADP'), ('a', 'DET'), ('strong', 'ADJ'), ('emotion', 'NOUN'), ('in', 'ADP'), ('a', 'DET'), ('nature', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('his', 'PRON'), ('.', 'PUNCT'), ('And', 'CCONJ'), ('\\n', 'SPACE'), ('yet', 'ADV'), ('there', 'PRON'), ('was', 'VERB'), ('but', 'CCONJ'), ('one', 'NUM'), ('woman', 'NOUN'), ('to', 'ADP'), ('him', 'PRON'), (',', 'PUNCT'), ('and', 'CCONJ'), ('that', 'DET'), ('woman', 'NOUN'), ('was', 'AUX'), ('the', 'DET'), ('late', 'ADJ'), ('Irene', 'PROPN'), ('\\n', 'SPACE'), ('Adler', 'PROPN'), (',', 'PUNCT'), ('of', 'ADP'), ('dubious', 'ADJ'), ('and', 'CCONJ'), ('questionable', 'ADJ'), ('memory', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag_nltk(text):\n",
        "  words = word_tokenize_nltk(text)\n",
        "  words_with_pos = nltk.pos_tag(words)\n",
        "  return words_with_pos"
      ],
      "metadata": {
        "id": "J5rnWQk9vFzY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEQXJLVsvpL-",
        "outputId": "19e6c487-99e5-4808-eefd-7148783daa7d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_with_pos = pos_tag_nltk(sherlock_text)\n",
        "print(words_with_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4blrradwFzO",
        "outputId": "cbd893fe-c174-4736-922b-a70650e284dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('To', 'TO'), ('Sherlock', 'NNP'), ('Holmes', 'NNP'), ('she', 'PRP'), ('is', 'VBZ'), ('always', 'RB'), ('_the_', 'JJ'), ('woman', 'NN'), ('.', '.'), ('I', 'PRP'), ('have', 'VBP'), ('seldom', 'VBN'), ('heard', 'RB'), ('him', 'PRP'), ('mention', 'VB'), ('her', 'PRP'), ('under', 'IN'), ('any', 'DT'), ('other', 'JJ'), ('name', 'NN'), ('.', '.'), ('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.'), ('It', 'PRP'), ('was', 'VBD'), ('not', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('felt', 'VBD'), ('any', 'DT'), ('emotion', 'NN'), ('akin', 'NN'), ('to', 'TO'), ('love', 'VB'), ('for', 'IN'), ('Irene', 'NNP'), ('Adler', 'NNP'), ('.', '.'), ('All', 'DT'), ('emotions', 'NNS'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('one', 'CD'), ('particularly', 'RB'), (',', ','), ('were', 'VBD'), ('abhorrent', 'JJ'), ('to', 'TO'), ('his', 'PRP$'), ('cold', 'NN'), (',', ','), ('precise', 'NN'), ('but', 'CC'), ('admirably', 'RB'), ('balanced', 'VBD'), ('mind', 'NN'), ('.', '.'), ('He', 'PRP'), ('was', 'VBD'), (',', ','), ('I', 'PRP'), ('take', 'VBP'), ('it', 'PRP'), (',', ','), ('the', 'DT'), ('most', 'RBS'), ('perfect', 'JJ'), ('reasoning', 'NN'), ('and', 'CC'), ('observing', 'VBG'), ('machine', 'NN'), ('that', 'IN'), ('the', 'DT'), ('world', 'NN'), ('has', 'VBZ'), ('seen', 'VBN'), (',', ','), ('but', 'CC'), ('as', 'IN'), ('a', 'DT'), ('lover', 'NN'), ('he', 'PRP'), ('would', 'MD'), ('have', 'VB'), ('placed', 'VBN'), ('himself', 'PRP'), ('in', 'IN'), ('a', 'DT'), ('false', 'JJ'), ('position', 'NN'), ('.', '.'), ('He', 'PRP'), ('never', 'RB'), ('spoke', 'VBD'), ('of', 'IN'), ('the', 'DT'), ('softer', 'JJR'), ('passions', 'NNS'), (',', ','), ('save', 'VBP'), ('with', 'IN'), ('a', 'DT'), ('gibe', 'NN'), ('and', 'CC'), ('a', 'DT'), ('sneer', 'NN'), ('.', '.'), ('They', 'PRP'), ('were', 'VBD'), ('admirable', 'JJ'), ('things', 'NNS'), ('for', 'IN'), ('the', 'DT'), ('observer—excellent', 'NN'), ('for', 'IN'), ('drawing', 'VBG'), ('the', 'DT'), ('veil', 'NN'), ('from', 'IN'), ('men', 'NNS'), ('’', 'VBP'), ('s', 'JJ'), ('motives', 'NNS'), ('and', 'CC'), ('actions', 'NNS'), ('.', '.'), ('But', 'CC'), ('for', 'IN'), ('the', 'DT'), ('trained', 'JJ'), ('reasoner', 'NN'), ('to', 'TO'), ('admit', 'VB'), ('such', 'JJ'), ('intrusions', 'NNS'), ('into', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('delicate', 'NN'), ('and', 'CC'), ('finely', 'RB'), ('adjusted', 'VBD'), ('temperament', 'NN'), ('was', 'VBD'), ('to', 'TO'), ('introduce', 'VB'), ('a', 'DT'), ('distracting', 'NN'), ('factor', 'NN'), ('which', 'WDT'), ('might', 'MD'), ('throw', 'VB'), ('a', 'DT'), ('doubt', 'NN'), ('upon', 'IN'), ('all', 'PDT'), ('his', 'PRP$'), ('mental', 'JJ'), ('results', 'NNS'), ('.', '.'), ('Grit', 'NNP'), ('in', 'IN'), ('a', 'DT'), ('sensitive', 'JJ'), ('instrument', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('crack', 'NN'), ('in', 'IN'), ('one', 'CD'), ('of', 'IN'), ('his', 'PRP$'), ('own', 'JJ'), ('high-power', 'NN'), ('lenses', 'NNS'), (',', ','), ('would', 'MD'), ('not', 'RB'), ('be', 'VB'), ('more', 'RBR'), ('disturbing', 'JJ'), ('than', 'IN'), ('a', 'DT'), ('strong', 'JJ'), ('emotion', 'NN'), ('in', 'IN'), ('a', 'DT'), ('nature', 'NN'), ('such', 'JJ'), ('as', 'IN'), ('his', 'PRP$'), ('.', '.'), ('And', 'CC'), ('yet', 'RB'), ('there', 'EX'), ('was', 'VBD'), ('but', 'CC'), ('one', 'CD'), ('woman', 'NN'), ('to', 'TO'), ('him', 'PRP'), (',', ','), ('and', 'CC'), ('that', 'IN'), ('woman', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('late', 'JJ'), ('Irene', 'NNP'), ('Adler', 'NNP'), (',', ','), ('of', 'IN'), ('dubious', 'JJ'), ('and', 'CC'), ('questionable', 'JJ'), ('memory', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing STOP Words**\n"
      ],
      "metadata": {
        "id": "KHaSpBGR3rOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewPzr2jy31Jf",
        "outputId": "fade91ff-309a-4d35-f728-854abe458df2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNU9IBAG361Q",
        "outputId": "869c0363-713a-4259-9ace-fc851f000bec"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize_nltk(sherlock_text)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8qOIPfc4CYl",
        "outputId": "95845034-194f-413d-ec18-387502c2bb83"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word for word in words if word not in stopwords.words('english')]\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwVUcBWq4N8s",
        "outputId": "d5a9855e-533f-4a5e-e127-73d4dd46de10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YIuD8ZN2BHkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chapter2 Grammar**\n",
        "\n",
        "#### <i>Difference between Singular and Plural </i>"
      ],
      "metadata": {
        "id": "qL3O5mX4BMlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"I have 5 birds\"\n",
        "doc = small_model(sample_text)\n",
        "for token in doc:\n",
        "  if (token.pos_ == \"NOUN\" and token.lemma_ != token.text):\n",
        "    print(token.text, \"plural\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWgazIYEBaV1",
        "outputId": "02940d81-b71b-498f-c9cc-603e0243136f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "birds plural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token,token.morph.get(\"Number\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNMD-9VoGF9y",
        "outputId": "30db40fd-a26b-49a9-9fa9-b3194ce94dae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ['Sing']\n",
            "have []\n",
            "5 []\n",
            "birds ['Plur']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Noun_number(Enum):\n",
        "  SINGULAR = 1\n",
        "  PLURAL = 2"
      ],
      "metadata": {
        "id": "D88ezRt3HGCW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nouns_number(text, model, method=\"lemma\"):\n",
        "  nouns = []\n",
        "  doc = model(text)\n",
        "  for token in doc:\n",
        "    if (token.pos_ == \"NOUN\"):\n",
        "      if method == \"lemma\":\n",
        "        if token.lemma_ != token.text:\n",
        "          nouns.append((token.text,Noun_number.PLURAL))\n",
        "        else:\n",
        "          nouns.append((token.text,Noun_number.SINGULAR))\n",
        "      elif method == \"morph\":\n",
        "        if token.morph.get(\"Number\") == \"Sing\":\n",
        "          nouns.append((token.text,Noun_number.PLURAL))\n",
        "        else:\n",
        "          nouns.append((token.text,Noun_number.SINGULAR))\n",
        "  print(nouns)\n",
        "  return nouns"
      ],
      "metadata": {
        "id": "Rzz5ykRYHH2A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Three geese crossed the road\"\n",
        "nouns = get_nouns_number(text, small_model, \"morph\")\n",
        "nouns = get_nouns_number(text, small_model, \"lemma\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZEEIjdlHkJ-",
        "outputId": "4c069538-4465-4f5c-f540-71f6da3e65db"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('geese', <Noun_number.SINGULAR: 1>), ('road', <Noun_number.SINGULAR: 1>)]\n",
            "[('geese', <Noun_number.PLURAL: 2>), ('road', <Noun_number.SINGULAR: 1>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "texts = [\"book\", \"goose\", \"pen\", \"point\", \"deer\"]\n",
        "blob_objs = [TextBlob(text) for text in texts]\n",
        "plurals = [blob_obj.words.pluralize()[0] for blob_obj in blob_objs]\n",
        "print(\"Plurals:\",plurals)\n",
        "blob_objs = [TextBlob(text) for text in plurals]\n",
        "singulars = [blob_obj.words.singularize()[0] for blob_obj in blob_objs]\n",
        "print(singulars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbvrRvfiIbSE",
        "outputId": "1a6c78d8-15b7-49f9-ecff-8c1766fcba89"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plurals: ['books', 'geese', 'pens', 'points', 'deer']\n",
            "['book', 'goose', 'pen', 'point', 'deer']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding root of the sentence"
      ],
      "metadata": {
        "id": "q1TGQdAO0WqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I have seldom heard him mention her under any other name.'\n",
        "def print_dependencies(sentence, model):\n",
        "  doc = model(sentence)\n",
        "  for token in doc:\n",
        "    print(token.text, \"\\t\", token.dep_, \"\\t\",spacy.explain(token.dep_))\n",
        "print_dependencies(sentence, small_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GQFgtg7ybPW",
        "outputId": "56dabfa0-051b-49ea-c8f5-dfd9f086d2f2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I \t nsubj \t nominal subject\n",
            "have \t aux \t auxiliary\n",
            "seldom \t advmod \t adverbial modifier\n",
            "heard \t ROOT \t root\n",
            "him \t nsubj \t nominal subject\n",
            "mention \t ccomp \t clausal complement\n",
            "her \t dobj \t direct object\n",
            "under \t prep \t prepositional modifier\n",
            "any \t det \t determiner\n",
            "other \t amod \t adjectival modifier\n",
            "name \t pobj \t object of preposition\n",
            ". \t punct \t punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_ancestors(sentence, model):\n",
        "  doc = model(sentence)\n",
        "  for token in doc:\n",
        "    print(token.text, [t.text for t in token.ancestors])\n",
        "print_ancestors(sentence, small_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_L9CKqpzwFA",
        "outputId": "a837667a-38df-4b02-f260-6648b303f6d9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ['heard']\n",
            "have ['heard']\n",
            "seldom ['heard']\n",
            "heard []\n",
            "him ['mention', 'heard']\n",
            "mention ['heard']\n",
            "her ['mention', 'heard']\n",
            "under ['mention', 'heard']\n",
            "any ['name', 'under', 'mention', 'heard']\n",
            "other ['name', 'under', 'mention', 'heard']\n",
            "name ['under', 'mention', 'heard']\n",
            ". ['heard']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Childeren (Dependents)"
      ],
      "metadata": {
        "id": "y-XEgAAxzH9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_children(sentence, model):\n",
        "  doc = model(sentence)\n",
        "  for token in doc:\n",
        "    print(token.text,[t.text for t in token.children])\n",
        "print_children(sentence, small_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE02HmsDr_y0",
        "outputId": "a734e393-12ec-44af-c11b-9ec2506b5fb5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I []\n",
            "have []\n",
            "seldom []\n",
            "heard ['I', 'have', 'seldom', 'mention', '.']\n",
            "him []\n",
            "mention ['him', 'her', 'under']\n",
            "her []\n",
            "under ['name']\n",
            "any []\n",
            "other []\n",
            "name ['any', 'other']\n",
            ". []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Left and Right Dependents"
      ],
      "metadata": {
        "id": "HYoE9BC_zOGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_lefts_and_rights(sentence, model):\n",
        "  doc = model(sentence)\n",
        "  for token in doc:\n",
        "    print(token.text,[t.text for t in token.lefts],[t.text for t in token.rights])\n",
        "print_lefts_and_rights(sentence, small_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alD_UfHOymIT",
        "outputId": "ce28009e-4237-4a0a-8d35-cf1fe0b8cde4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I [] []\n",
            "have [] []\n",
            "seldom [] []\n",
            "heard ['I', 'have', 'seldom'] ['mention', '.']\n",
            "him [] []\n",
            "mention ['him'] ['her', 'under']\n",
            "her [] []\n",
            "under [] ['name']\n",
            "any [] []\n",
            "other [] []\n",
            "name ['any', 'other'] []\n",
            ". [] []\n"
          ]
        }
      ]
    }
  ]
}